=========================================================================================
README: A Quantum Plugin Framework for Supporting L2 Networks Spannning Multiple Switches
=========================================================================================

:Author:  Sumit Naiksatam, Ram Durairaj, Mark Voelker, Edgar Magana, Shweta Padubidri,
          Rohit Agarwalla, Ying Liu, Debo Dutta
:Contact: netstack@lists.launchpad.net
:Web site: https://launchpad.net/~cisco-openstack
:Copyright: 2011 Cisco Systems, Inc.

.. contents::

Introduction
------------

This plugin implementation provides the following capabilities
to help you take your Layer 2 network for a Quantum leap:

* A reference implementation for a Quantum Plugin Framework
(For details see: http://wiki.openstack.org/quantum-multi-switch-plugin)
* Supports multiple switches in the network
* Supports multiple models of switches concurrently
* Supports use of multiple L2 technologies
* Supports Cisco UCS blade servers with M81KR Virtual Interface Cards 
  (aka "Palo adapters") via 802.1Qbh.
* Supports the Cisco Nexus family of switches.

It does not provide:

* A hologram of Al that only you can see.
* A map to help you find your way through time.
* A cure for amnesia or your swiss-cheesed brain.

Let's leap in!

Pre-requisites
--------------
(The following are necessary only when using the UCS and/or Nexus devices in your system.
If you plan to just leverage the plugin framework, you do not need these.)
* One or more UCS B200 series blade servers with M81KR VIC (aka 
  Palo adapters) installed.
* UCSM 2.0 (Capitola) Build 230 or above.
* OpenStack Cactus release installation (additional patch is required,
  details follow in this document)
* RHEL 6.1 (as of this writing, UCS only officially supports RHEL, but
  it should be noted that Ubuntu support is planned in coming releases as well)
  ** Package: python-configobj-4.6.0-3.el6.noarch (or newer)
  ** Package: python-routes-1.12.3-2.el6.noarch (or newer)

If you are using a Nexus switch in your topology, you'll need the following 
NX-OS version and packages to enable Nexus support:
* NX-OS 5.2.1 (Delhi) Build 69 or above.
* paramiko library - SSHv2 protocol library for python
  ** To install on RHEL 6.1, run: yum install python-paramiko
* ncclient v0.3.1 - Python library for NETCONF clients 
  ** RedHat does not provide a package for ncclient in RHEL 6.1. Here is how
     to get it, from your shell prompt do:

     git clone git@github.com:ddutta/ncclient.git 
     sudo python ./setup.py install

  ** For more information of ncclient, see:
     http://schmizz.net/ncclient/

To verify the version of any package you have installed on your system,
run "rpm -qav | grep <package name>", where <package name> is the 
package you want to query (for example: python-routes).

Note that you can get access to recent versions of the packages above
and other OpenStack software packages by adding a new repository to 
your yum configuration.  To do so, edit or create 
/etc/yum.repos.d/openstack.repo and add the following:

[openstack-deps]
name=OpenStack Nova Compute Dependencies
baseurl=http://yum.griddynamics.net/yum/cactus/deps
enabled=1
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OPENSTACK

Then run "yum install python-routes".


Module Structure:
-----------------
* quantum/plugins/cisco/       - Contains the L2-Network Plugin Framework
                       /common - Modules common to the entire plugin
                       /conf   - All configuration files
                       /db     - Persistence framework
                       /models - Class(es) which tie the logical abstractions
                                 to the physical topology
                       /nexus  - Nexus-specific modules
                       /segmentation - Implementation of segmentation manager,
                                       e.g. VLAN Manager
                       /tests  - Tests specific to this plugin
                       /ucs    - UCS-specific modules


Plugin Installation Instructions
----------------------------------
1.  Make a backup copy of quantum/quantum/plugins.ini.

2.  Edit quantum/quantum/plugins.ini and edit the "provider" entry to point
    to the L2Network-plugin:

provider = quantum.plugins.cisco.l2network_plugin.L2Network

3.  Configure your OpenStack installation to use the 802.1qbh VIF driver and
    Quantum-aware scheduler by editing the /etc/nova/nova.conf file with the
    following entries:

--scheduler_driver=quantum.plugins.cisco.nova.quantum_aware_scheduler.QuantumScheduler
--quantum_host=127.0.0.1
--quantum_port=9696
--libvirt_vif_driver=quantum.plugins.cisco.nova.vifdirect.Libvirt802dot1QbhDriver
--libvirt_vif_type=802.1Qbh
 

4.  If you want to turn on support for Cisco Nexus switches:
    4a.  Uncomment the nexus_plugin property in 
         quantum/plugins/cisco/conf/plugins.ini to read:

nexus_plugin=quantum.plugins.cisco.nexus.cisco_nexus_plugin.NexusPlugin

    4b.  Enter the relevant configuration in the 
         quantum/plugins/cisco/conf/nexus.ini file.  Example:

[SWITCH]
# Change the following to reflect the IP address of the Nexus switch.
# This will be the address at which Quantum sends and receives configuration
# information via SSHv2.
nexus_ip_address=10.0.0.1
# Port numbers on the Nexus switch to each one of the UCSM 6120s is connected
# Use shortened interface syntax, e.g. "1/10" not "Ethernet1/10".
nexus_first_port=1/10
nexus_second_port=1/11
#Port number where SSH will be running on the Nexus switch.  Typically this is 22
#unless you've configured your switch otherwise.
nexus_ssh_port=22

[DRIVER]
name=quantum.plugins.cisco.nexus.cisco_nexus_network_driver.CiscoNEXUSDriver

    4c.  Make sure that SSH host key of the Nexus switch is known to the
         host on which you are running the Quantum service.  You can do
         this simply by logging in to your Quantum host as the user that
         Quantum runs as and SSHing to the switch at least once.  If the
         host key changes (e.g. due to replacement of the supervisor or
         clearing of the SSH config on the switch), you may need to repeat
         this step and remove the old hostkey from ~/.ssh/known_hosts.
         
5.  Plugin Persistence framework setup:
	5a.  Create quantum_l2network database in mysql with the following command -
	
mysql -u<mysqlusername> -p<mysqlpassword> -e "create database quantum_l2network"

	5b.  Enter the quantum_l2network database configuration info in the 
         quantum/plugins/cisco/conf/db_conn.ini file.
         
    5c.  If there is a change in the plugin configuration, service would need 
         to be restarted after dropping and re-creating the database using
         the following commands -

mysql -u<mysqlusername> -p<mysqlpassword> -e "drop database quantum_l2network"
mysql -u<mysqlusername> -p<mysqlpassword> -e "create database quantum_l2network"

6.  Verify that you have the correct credentials for each IP address listed
    in quantum/plugins/cisco/conf/credentials.ini.  Example:

# Provide the UCSM credentials
# UCSM IP address, username and password.
[10.0.0.2]
username=admin
password=mySecretPasswordForUCSM

# Provide the Nova DB credentials.
# The IP address should be the same as in nova.ini.
[10.0.0.3]
username=nova
password=mySecretPasswordForNova

# Provide the Nexus credentials, if you are using Nexus switches.
# If not this will be ignored.
[10.0.0.1]
username=admin
password=mySecretPasswordForNexus

7.  Configure the UCS systems' information in your deployment by editing the
    quantum/plugins/cisco/conf/ucs_inventory.ini file. You can configure multiple
    UCSMs per deployment, multiple chasses per UCSM, and multiple blades per
    chassis. Chassis ID and blade ID can be obtained from the UCSM (they will
    typically numbers like 1, 2, 3, etc.

[ucsm-1]
ip_address = <put_ucsm_ip_address_here>
[[chassis-1]]
chassis_id = <put_the_chassis_id_here>
[[[blade-1]]]
blade_id = <put_blade_id_here>
host_name = <put_hostname_here>
[[[blade-2]]] 
blade_id = <put_blade_id_here> 
host_name = <put_hostname_here> 
[[[blade-3]]]
blade_id = <put_blade_id_here>
host_name = <put_hostname_here>
         
[ucsm-2]
ip_address = <put_ucsm_ip_address_here>
[[chassis-1]]
chassis_id = <put_the_chassis_id_here>
[[[blade-1]]]
blade_id = <put_blade_id_here>
host_name = <put_hostname_here>
[[[blade-2]]]
blade_id = <put_blade_id_here>
host_name = <put_hostname_here>


8.  Start the Quantum service.  If something doesn't work, verify that
    your configuration of each of the above files hasn't gone a little kaka.
    Once you've put right what once went wrong, leap on.


How to test the installation
----------------------------
The unit tests are located at quantum/plugins/cisco/tests/unit. They can be
executed from the main folder using the run_tests.sh or to get a more detailed
result the quantum/plugins/cisco/run_tests.py script.

1. Testing the core API (without UCS/Nexus/RHEL hardware, and can be run on
   Ubuntu):
   First disable all device-specific plugins by commenting out the entries in:
   quantum/plugins/cisco/conf/plugins.ini
   Then run the test script:

   Set the environment variable PLUGIN_DIR to the location of the plugin 
   directory. This is manadatory if the run_tests.sh script is used.

   In bash : export PLUGIN_DIR=quantum/plugins/cisco
      tcsh/csh : setenv PLUGIN_DIR quantum/plugins/cisco
   ./run_tests.sh quantum.plugins.cisco.tests.unit.test_l2networkApi

   or

   python quantum/plugins/cisco/run_tests.py 
	quantum.plugins.cisco.tests.unit.test_l2networkApi

2. Specific Plugin unit test (needs environment setup as indicated in the 
   pre-requisites):

   In bash : export PLUGIN_DIR=quantum/plugins/cisco
      tcsh/csh : setenv PLUGIN_DIR quantum/plugins/cisco
   ./run_tests.sh quantum.plugins.cisco.tests.unit.<name_of_the file>

   or 

   python <path to the plugin directory>/run_tests.py 
          quantum.plugins.cisco.tests.unit.<name_of_the file>
   E.g.:

   python quantum/plugins/cisco/run_tests.py 
          quantum.plugins.cisco.tests.unit.test_ucs_plugin

3. All unit tests (needs environment setup as indicated in the pre-requisites):
   
   In bash : export PLUGIN_DIR=quantum/plugins/cisco
      tcsh/csh : setenv PLUGIN_DIR quantum/plugins/cisco

   ./run_tests.sh quantum.plugins.cisco.tests.unit

   or 

   python quantum/plugins/cisco/run_tests.py quantum.plugins.cisco.tests.unit

4. Testing the Extension API
   The script is placed alongwith the other cisco unit tests. The location may
   change later.
   Location quantum/plugins/cisco/tests/unit/test_cisco_extension.py

   The script can be executed by :
    ./run_tests.sh quantum.plugins.cisco.tests.unit.test_cisco_extension

    or

    python run_tests.py quantum.plugins.cisco.tests.unit.test_cisco_extension

   To run specific tests
   python run_tests.py 
    quantum.plugins.cisco.tests.unit.test_cisco_extension:<ClassName>.<funcName>

Bingo bango bongo!  That's it!  Thanks for taking the leap into Quantum.

...Oh, boy!
